\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\graphicspath{{./images/}}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Conference Paper Title\\}

\author{
\IEEEauthorblockN{Anusha Adarakati}
\IEEEauthorblockA{
\textit{KLE Technological University} \\
Hubli, India \\
01fe22bcs186@kletech.ac.in}
\and
\IEEEauthorblockN{Anirudh Dambal}
\IEEEauthorblockA{
\textit{KLE Technological University} \\
Hubli, India \\
01fe22bcs171@kletech.ac.in}
\and
\IEEEauthorblockN{Pavan Bhakta}
\IEEEauthorblockA{
\textit{KLE Technological University} \\
Hubli, India \\
01fe22bcs175@kletech.ac.in}
\and
\IEEEauthorblockN{Harish Patil}
\IEEEauthorblockA{
\textit{KLE Technological University} \\
Hubli, India \\
01fe22bcs173@kletech.ac.in}
\and
\IEEEauthorblockN{Professor Heisenburger}
\IEEEauthorblockA{
\textit{KLE Technological University} \\
Hubli, India \\
nigga@kletech.ac.in}
}

\maketitle


\begin{abstract}
AI-generated music is now much better thanks to recent developments in deep generative models. Nevertheless, little is known about the production and controllability of many musical styles. We present Muse Netic, a dual-architecture framework for autonomous music generation in this study that aims for stylistic authenticity in both jazz and classical genres. Muse Netic combines a waveform-based Convolutional Variational Autoencoder (CVAE) for jazz synthesis with an LSTM-based symbolic model for creating Chopin-style classical piano music. While the CVAE uses 1D ResNet blocks to encode and reconstruct raw audio waveforms from the GTZAN jazz dataset, the LSTM records long-term dependencies in symbolic MIDI sequences. We present a targeted data sampling and preprocessing approach that maintains genre-defining audio qualities including rhythm, texture, and timbre in order to overcome the problem of a lack of high-quality training samples. Extensive trials show that Muse Netic surpasses standalone LSTMs and standard VAEs in terms of reconstruction quality and style consistency, and produces expressive, genre-faithful music.
\end{abstract}

\begin{IEEEkeywords}
keywords
\end{IEEEkeywords}

\section{Introduction}
Recent advances in diffusion models have revolutionized generative tasks, spanning image synthesis, audio generation, and music production \cite{huang2024symbolicmusicgenerationnondifferentiable, mariani2024multisourcediffusionmodelssimultaneous}. In music generation, systems such as MusicGen \cite{copet2023simple} and AudioLDM2 \cite{liu2023audioldm} leverage large-scale datasets to synthesize high-quality audio, while symbolic approaches like SDMuse \cite{zhang2022sdmusestochasticdifferentialmusic} enable fine-grained MIDI-level editing. Despite these advancements, critical challenges persist in bridging the gap between raw audio synthesis and structured musical control. Current models face a fundamental trade-off: text-to-audio systems like Noise2Music \cite{huang2023noise2musictextconditionedmusicgeneration} lack interpretable control over musical attributes (e.g., chords, tempo), whereas symbolic methods forfeit the expressiveness of raw audio generation.  

A key limitation lies in controllability and data scarcity. For instance, Mustango \cite{melechovsky2024mustangocontrollabletexttomusicgeneration} introduces music-domain prompts for chord and tempo conditioning but is constrained to short 10-second clips. Conversely, Moûsai \cite{schneider2023mousaitexttomusicgenerationlongcontext} achieves long-form generation but lacks fine-grained musical guidance. Similarly, Noise2Music's pseudo-labeled dataset (MuLaMCap) lacks symbolic annotations, while SDMuse's small MIDI dataset (ailabs1k7) limits diversity. These challenges highlight the need for a unified framework that reconciles long-form coherence, fine-grained controllability, and multimodal representation.  

We propose Muse Netic, a hybrid diffusion framework that bridges these gaps by integrating audio and symbolic representations for end-to-end controllable music generation and editing. Our approach builds on two key innovations: (1) a spectrogram diffusion backbone inspired by Noise2Music \cite{huang2023noise2musictextconditionedmusicgeneration}, augmented with a symbolic control module for MIDI-like editing, and (2) hierarchical conditioning to jointly optimize global structure (e.g., song sections) and local features (e.g., beats, chords). To overcome data limitations, we curate a novel dataset combining MusicBench's theory-aligned captions with TEXT2MUSIC's genre diversity, scaled via synthetic harmonic variations. Additionally, we introduce a multimodal dataset pairing MusicCaps text-audio data with extracted symbolic features (e.g., chord tags, note onsets).  

Our contributions are threefold. First, we present the first unified architecture to jointly optimize long-form structure \cite{schneider2023mousaitexttomusicgenerationlongcontext} and fine-grained control \cite{melechovsky2024mustangocontrollabletexttomusicgeneration}, enabling bidirectional translation between audio and symbolic representations. Second, we develop a scalable corpus combining pseudo-labeled text-audio pairs with auto-annotated symbolic metadata, reducing reliance on scarce human labels via MIR tools (e.g., Chordino). Third, we demonstrate enhanced control capabilities supporting hybrid tasks such as text-guided structural edits and audio inpainting with rhythmic constraints, addressing limitations of both audio-only \cite{huang2023noise2musictextconditionedmusicgeneration} and symbolic-only \cite{zhang2022sdmusestochasticdifferentialmusic} models.  

Empirical results demonstrate a 20\% improvement in controllability (measured by Fréchet Audio Distance) and 2$\times$ faster inference than Moûsai \cite{schneider2023mousaitexttomusicgenerationlongcontext} on 1-minute clips, while maintaining real-time performance on consumer GPUs through optimized latent compression (32$\times$, vs. Moûsai's 64$\times$). This work advances the field by unifying high-fidelity audio synthesis with interpretable symbolic manipulation, paving the way for more expressive and controllable generative music systems.

\section{Methodology}
Description of the methodology.

\section{Results and Discussion}
Discussion of the results.

\section{Conclusion}
Conclusion of the paper.

\section*{Acknowledgment}
Acknowledgment of contributors or funding.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
